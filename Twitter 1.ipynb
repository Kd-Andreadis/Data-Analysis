{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1387be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR ONE TIME RUN\n",
    "# 2 ways of installing: Through anaconda navigator->environments etc & through terminal of a specific environment *it also opens at\n",
    "# anaconda navigator->environments->'play button'\n",
    "\n",
    "# for tweepy we need to add 'conda-forge' channel in the anaconda environment and update (channels). Then, we can find tweepy in the anaconda list of packages\n",
    "# for mysql.connector I add 'mysql-connector-python'\n",
    "# for matplotlib I can use the alternative way of intalling: I write 'pip install matplotlib' in conda prompt\n",
    "# for textblob I write in prompt 'pip install textblob'\n",
    "# for pandas I write in prompt 'pip install pandas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c965d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # ??\n",
    "import re\n",
    "import tweepy\n",
    "import mysql.connector\n",
    "from mysql.connector import Error   #Gia na fortwsw ta eidika errors gia sql connection\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import string  # ??\n",
    "import nltk  # ??\n",
    "from nltk.stem import WordNetLemmatizer  # ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af4b59",
   "metadata": {},
   "source": [
    "# Tweets Fetching\n",
    "Needed steps:\n",
    "- Create a class inheriting from StreamListener\n",
    "- Using that class create a Stream object\n",
    "- Connect to the Twitter API using the Stream.\n",
    "\n",
    "Notes: <br>\n",
    "- The Key Differences Between Streaming APIs and REST APIs: https://www.nstream.io/the-key-differences-between-streaming-apis-and-rest-apis/ <br>\n",
    "UNFORTUNATELY, Streaming tweets are not permitted in free and basic versions of Twitter Developer\n",
    "https://stackoverflow.com/questions/76445414/twitter-streams-api-returns-403\n",
    "\n",
    "- Tweepy docmentation:\n",
    "https://docs.tweepy.org/en/stable/index.html\n",
    "\n",
    "We will use Twitter API v2, which uses 'Client' method (or interface) that returns a Twitter API object<br>\n",
    "https://docs.tweepy.org/en/stable/client.html#tweepy.Client\n",
    "https://dev.to/twitterdev/a-comprehensive-guide-for-using-the-twitter-api-v2-using-tweepy-in-python-15d9\n",
    "\n",
    "Twitter Developer Portal\n",
    "https://developer.twitter.com/en/portal/dashboard\n",
    "\n",
    "Twitter queries\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9229b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHENTICATION of Twitter Client\n",
    "\n",
    "# # Configuration of the twitter account\n",
    "p_consumer_key = 'SbPzmphYe7DHRMB0QPnYCuj1D'\n",
    "p_consumer_secret = 'Wod9VZS49DHxKhXKc5TL6yO2yrzZpeakvUUH1bufrspbBt8ygh'\n",
    "p_access_token = '1485552804655685639-zjmpYuEc1AaZ1dQuLTdKaMyq72SQfW'\n",
    "p_access_token_secret = '829gYuJgv79BprhxxP1TFCvkJigL7mtFqtqFIwcvYu6xv'\n",
    "p_bearer_token = 'AAAAAAAAAAAAAAAAAAAAAEd1YgEAAAAAOH7PeGJv9rqSuF%2F2It5VB%2BX3S2g%3DPsP4UHNi0bwZEYoUmz5EwyAPRVny9bl9iChLNPFy0MvAw5Een2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7daedbe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m p_user_fields \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# tweets = p_client.search_recent_tweets(query=p_query, tweet_fields=p_tweet_fields, user_fields = p_user_fields, max_results=10)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mp_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_recent_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweet_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_tweet_fields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets\u001b[38;5;241m.\u001b[39mdata:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(user\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Case_studies\\lib\\site-packages\\tweepy\\client.py:1266\u001b[0m, in \u001b[0;36mClient.search_recent_tweets\u001b[1;34m(self, query, user_auth, **params)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"search_recent_tweets( \\\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;124;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;124;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;124;03m.. _Academic Research Project: https://developer.twitter.com/en/docs/projects\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/2/tweets/search/recent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpansions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedia.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplace.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoll.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msince_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msort_order\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweet.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muntil_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Case_studies\\lib\\site-packages\\tweepy\\client.py:129\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m(), json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ):\n\u001b[0;32m    127\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Case_studies\\lib\\site-packages\\tweepy\\client.py:100\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(response)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(response)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFound(response)\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal."
     ]
    }
   ],
   "source": [
    "# USE CLIENT class to search for tweets (not real time tweets)\n",
    "\n",
    "p_client = tweepy.Client(bearer_token=p_bearer_token, consumer_key=p_consumer_key, consumer_secret=p_consumer_secret, access_token=p_access_token, access_token_secret=p_access_token_secret)\n",
    "p_query = 'python'\n",
    "# p_tweet_fields = ['context_annotations', 'created_at','text','lang','geo','id']\n",
    "p_tweet_fields = ['text']\n",
    "p_user_fields = ['name', 'id', 'location','username']\n",
    "\n",
    "# tweets = p_client.search_recent_tweets(query=p_query, tweet_fields=p_tweet_fields, user_fields = p_user_fields, max_results=10)\n",
    "tweets = p_client.search_recent_tweets(query=p_query, tweet_fields=p_tweet_fields)\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    print(user.name)\n",
    "\n",
    "\n",
    "\n",
    "# #each of those lists will contain the values for a column in the dataframe\n",
    "# list1=[]\n",
    "# list2=[]\n",
    "# list3=[]\n",
    "# list4=[]\n",
    "# list5=[]\n",
    "# list6=[]\n",
    "# list7=[]\n",
    "# list8=[]\n",
    "# list9=[]\n",
    "\n",
    "# # Pass the tweets' metadata in lists\n",
    "# for tweet in tweets:\n",
    "#     list1.append(tweet.text)\n",
    "#     list2.append(tweet.user.screen_name)\n",
    "#     list3.append(tweet.created_at)\n",
    "#     list4.append(tweet.retweet_count)\n",
    "#     list5.append(tweet.user.location)\n",
    "#     list6.append(tweet.user.followers_count)\n",
    "#     list7.append(tweet.user.statuses_count)\n",
    "#     list8.append(tweet.entities['hashtags'])\n",
    "\n",
    "# # Create the dataframe\n",
    "# data = pd.DataFrame(list2, columns=[\"user\"])\n",
    "# data.insert(loc=1, column='tweet', value=list1)\n",
    "# data.insert(loc=2, column='date', value=list3)\n",
    "# data.insert(loc=3, column='n_ret', value=list4)\n",
    "# data.insert(loc=4, column='loc', value=list5)\n",
    "# data.insert(loc=5, column='n_fol', value=list6)\n",
    "# data.insert(loc=6, column='n_twe', value=list7)\n",
    "# data.insert(loc=7, column='hashtags', value=list8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5a273",
   "metadata": {},
   "source": [
    "# CONNECT TO DATABASE TO STORE RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c292b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'case_st_twit'\n",
    "\n",
    "# CONNECT TO DATABASE AND CREATE THE TABLE THAT WE WILL STORE THE TWEETS\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database = db_name , user='root', password='')\n",
    "    if db.is_connected():\n",
    "        print('Connected to MySQL Database')\n",
    "        cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "        # Creating the table of database\n",
    "        q = \"CREATE table r_tweets( \\\n",
    "                username VARCHAR(30) PRIMARY KEY, \\\n",
    "                tweet VARCHAR(280), \\\n",
    "                t_date VARCHAR(30), \\\n",
    "                n_ret INT(3), \\\n",
    "                loc VARCHAR(20), \\\n",
    "                n_fol INT(9), \\\n",
    "                n_twe INT(3), \\\n",
    "                hashtags VARCHAR(50), \\\n",
    "                commit;\"\n",
    "        cur.execute(q)\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n",
    "    print('Database connection closed!')\n",
    "\n",
    "# CONNECT TO DATABASE TO STORE THE FETCHED TWEETS AS RAW DATA\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database= db_name, user='root', password='')\n",
    "    if db.is_connected():\n",
    "        cur = db.cursor()\n",
    "\n",
    "        for row in range(len(list1)):\n",
    "            # Before running the sql commands, we clean the text data from apostrophes <'>, in order to avoid losing data due to faulty comments in sql commands\n",
    "            q = \"INSERT INTO tweets (username, tweet, t_date, n_ret, loc, n_fol, n_twe, hashtags) VALUES ('\" \\\n",
    "            + re.sub(r'[\\']', ' ',str(list2[row])) + \"', '\" + re.sub(r'[\\']', ' ',str(list1[row])) + \"', '\" + str(list3[row]) + \\\n",
    "            \"', '\" + str(list4[row]) + \"', '\" + re.sub(r'[\\']', ' ',str(list5[row])) + \"', '\" + str(list6[row]) + \\\n",
    "            \"', '\" + str(list7[row]) + \"', '\"  + re.sub(r'[\\']', ' ',str(list8[row])) + \\\n",
    "            \"');\"\n",
    "\n",
    "            try:\n",
    "                print(q)\n",
    "                cur.execute(q)\n",
    "            except Error as e:\n",
    "                print(e)\n",
    "\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831fb35b",
   "metadata": {},
   "source": [
    "# CLEANING OF THE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33933c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data from database\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database = db_name, user='root', password='')\n",
    "\n",
    "    # Initialization for storing the tweets in pandas object 'data'\n",
    "    data = []\n",
    "    columns = ['user', 'tweet', 'date', 'n_ret', 'loc', 'n_fol', 'n_twe', 'hashtags']\n",
    "    if db.is_connected():\n",
    "        cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "        q = \"SELECT * FROM tweets;\"\n",
    "        cur.execute(q)\n",
    "        #Create the dataframe with all the data from database\n",
    "        data = pd.DataFrame(cur.fetchall(), columns=columns)\n",
    "\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n",
    "\n",
    "\n",
    "# Remove duplicate users. We want one tweet from every user\n",
    "data.drop_duplicates(subset =\"user\", keep = False, inplace = True)\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "list9=[]\n",
    "for row in data.index:\n",
    "    # Remove punctuations\n",
    "    cleaned_tweet = ''.join(re.sub (\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])| (\\w+:\\ / \\ / \\S+)\", \" \", data['tweet'][row]))\n",
    "    # Remove the words with the @ symbol\n",
    "    cleaned_tweet = re.sub ('@\\S+', '', cleaned_tweet)\n",
    "    # Remove the Url's\n",
    "    cleaned_tweet = re.sub('((www.[^s]+)|(https?://[^s]+))', ' ', cleaned_tweet)\n",
    "    # Remove the hashtags\n",
    "    cleaned_tweet = re.sub ('#\\S+', '', cleaned_tweet)\n",
    "    # Remove the change row\n",
    "    cleaned_tweet = re.sub ('\\\\n', '', cleaned_tweet)\n",
    "    # Remove Stopwords\n",
    "    stopwords = set([\"\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"A\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"])\n",
    "    cleaned_tweet = ' '.join([word for word in str(cleaned_tweet).split() if word not in stopwords])\n",
    "\n",
    "    # Calculate the polarity of the sentiment analysis using textblob and store the results in the sentiment column of the dataframe\n",
    "    analysis = TextBlob(cleaned_tweet)\n",
    "    list9.append(analysis.sentiment.polarity)\n",
    "\n",
    "    if (analysis.sentiment.polarity == 0):\n",
    "        neutral += 1\n",
    "    elif (analysis.sentiment.polarity < 0):\n",
    "        negative += 1\n",
    "    elif (analysis.sentiment.polarity > 0):\n",
    "        positive += 1\n",
    "\n",
    "data.insert(loc=8, column='polarity', value = list9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b224cbfe",
   "metadata": {},
   "source": [
    "# STORE THE CLEANED TWEETS IN DATABASE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a856888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A NEW TABLE IN DATABASE cl_tweets\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database='twit_sent_an', user='root', password='')\n",
    "    if db.is_connected():\n",
    "        cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "        q = \"CREATE table cl_tweets( \\\n",
    "                username VARCHAR(30) PRIMARY KEY, \\\n",
    "                tweet VARCHAR(280), \\\n",
    "                t_date VARCHAR(30), \\\n",
    "                n_ret INT(3), \\\n",
    "                loc VARCHAR(20), \\\n",
    "                n_fol INT(9), \\\n",
    "                n_twe INT(3), \\\n",
    "                hashtags VARCHAR(50), \\\n",
    "                sentiment FLOAT(4,3)); \\\n",
    "                commit;\"\n",
    "        cur.execute(q)\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n",
    "\n",
    "\n",
    "# STORE THE CLEANED TWEETS WITH THE SENTIMENT ANALYSIS RESULTS IN A NEW TABLE IN DATABASE cl_tweets\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database='twit_sent_an', user='root', password='')\n",
    "    if db.is_connected():\n",
    "        print('Connected to MySQL Database')\n",
    "        cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "        for row in data.index:\n",
    "            q = \"INSERT INTO cl_tweets (username, tweet, t_date, n_ret, loc, n_fol, n_twe, hashtags, sentiment) VALUES ('\" \\\n",
    "            + str(data['user'][row]) + \"', '\" + str(data['tweet'][row]) + \"', '\" + str(data['date'][row]) + \\\n",
    "            \"', '\" + str(data['n_ret'][row]) + \"', '\" + str(data['loc'][row]) + \"', '\" + str(data['n_fol'][row]) + \\\n",
    "            \"', '\" + str(data['n_twe'][row]) + \"', '\"  + str(data['hashtags'][row]) + \\\n",
    "            \"', '\" + str(data['polarity'][row]) + \"');\"\n",
    "\n",
    "            cur.execute(q)\n",
    "\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n",
    "    print('Database connection closed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mega_positive=0\n",
    "mega_negative=0\n",
    "mega_neutral=0\n",
    "mini_positive=0\n",
    "mini_negative=0\n",
    "mini_neutral=0\n",
    "\n",
    "list10=[]\n",
    "#list 10 is going to accept 2 values (1 for the people that have more followers than average and 0\n",
    "#for those who have followers less than average)\n",
    "mean_followers = sum(data['n_fol'])/ len(data['n_fol'])\n",
    "for element in data['n_fol']:\n",
    "    if element > mean_followers:\n",
    "        list10.append(1)\n",
    "    else:\n",
    "        list10.append(0)\n",
    "\n",
    "data.insert(loc=9, column='weight', value=list10)\n",
    "\n",
    "# 9th and 10th column in the dataframe contain the polarity value and the corresponding weight whether\n",
    "#a user has followers above average or not\n",
    "#So mega_positive will get +1 when we meet a user that has followers above average and polarity > 0\n",
    "#Thus, mini positive will get +1 when we meet a user that has followers below average and polarity > 0 etc.\n",
    "for index,row in data.iterrows():\n",
    "    if row['polarity'] > 0 and row['weight'] == 1:\n",
    "        mega_positive += 1\n",
    "    elif row['polarity'] > 0 and row['weight'] == 0:\n",
    "        mini_positive += 1\n",
    "    elif row['polarity'] < 0 and row['weight'] == 1:\n",
    "        mega_negative += 1\n",
    "    elif row['polarity'] < 0 and row['weight'] == 0:\n",
    "        mini_negative += 1\n",
    "    elif row['polarity'] == 0 and row['weight'] == 1:\n",
    "        mega_neutral += 1\n",
    "    else:\n",
    "        mini_neutral += 1\n",
    "\n",
    "\n",
    "#we want to have the percentage each opinion covers in the pie we are going to create\n",
    "positive =(positive/ noOfSearchTerms)*100\n",
    "negative =(negative/ noOfSearchTerms)*100\n",
    "neutral =(neutral/ noOfSearchTerms)*100\n",
    "\n",
    "final_positive = mega_positive*0.7 + mini_positive*0.3\n",
    "final_negative = mega_negative*0.7 + mini_negative*0.3\n",
    "final_neutral = mega_neutral*0.7 + mini_neutral*0.3\n",
    "\n",
    "\n",
    "print(\"how people are reacting on \" + searchTerm + \"by analyzing\" + str(noOfSearchTerms) + \"Tweets.\")\n",
    "\n",
    "if (polarity == 0):\n",
    "    print(\"Neutral\")\n",
    "elif (polarity < 0):\n",
    "    print(\"Negative\")\n",
    "else:\n",
    "    print(\"Positive\")\n",
    "\n",
    "#plotting the polarity of objects\n",
    "labels = ['Positive [' +str(positive)+'%]', 'Neutral [' +str(neutral)+ '%]', 'Negative [' +str(negative) + '%]']\n",
    "\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['green', 'yellow', 'red']\n",
    "patches, text = plt.pie(sizes, colors=colors)\n",
    "\n",
    "plt.legend(patches, labels, loc='best')\n",
    "plt.title('what people think about ' +searchTerm+'by analyzing '+str(noOfSearchTerms)+'Tweets.')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#plotting the impact that people with more followers have on the objects\n",
    "labels1 = ['Positive ' +str(final_positive)+' impact', 'Neutral ' +str(final_neutral)+ ' impact', 'Negative ' +str(final_negative) + ' impact']\n",
    "\n",
    "sizes1 = [final_positive, final_neutral, final_negative]\n",
    "colors1= ['green', 'yellow', 'red']\n",
    "patches1, text1 = plt.pie(sizes, colors=colors)\n",
    "\n",
    "plt.legend(patches1, labels1, loc='best')\n",
    "plt.title('what people think about ' +searchTerm+'by analyzing '+str(noOfSearchTerms)+'Tweets.')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Dataset separation based on the location. Here we use two csv's tha contain some cities in USA and Europe\n",
    "abbUS = pd.read_csv('AbbrevUSA.csv')\n",
    "abbEU = pd.read_csv('AbbrevEU.txt')\n",
    "usa = []\n",
    "EU = []\n",
    "other = []\n",
    "for index,row in data.iterrows():\n",
    "    if row['loc'] in abbUS.values:\n",
    "        usa.append(index)\n",
    "    elif row['loc'] in abbEU.values:\n",
    "        EU.append(index)\n",
    "    elif row['loc']!= '':\n",
    "        other.append(index)\n",
    "\n",
    "# Dataset separation based on the platform\n",
    "net = []\n",
    "disney = []\n",
    "amazon = []\n",
    "net_sum = 0\n",
    "dis_sum = 0\n",
    "am_sum = 0\n",
    "for index,row in data.iterrows():\n",
    "    if 'netflix' in row['tweet'].lower():\n",
    "        net.append(index)\n",
    "        net_sum += row['polarity']\n",
    "    elif 'disney' in row['tweet'].lower():\n",
    "        disney.append(index)\n",
    "        dis_sum += row['polarity']\n",
    "    elif 'amazon' in row['tweet'].lower():\n",
    "        amazon.append(index)\n",
    "        am_sum += row['polarity']\n",
    "\n",
    "# Preperation of the data in order to plot the polarity of the tweets based on Location info\n",
    "usa_net_sum = 0\n",
    "usa_dis_sum = 0\n",
    "usa_am_sum = 0\n",
    "eu_net_sum = 0\n",
    "eu_dis_sum = 0\n",
    "eu_am_sum = 0\n",
    "oth_net_sum = 0\n",
    "oth_dis_sum = 0\n",
    "oth_am_sum = 0\n",
    "usa_net_len = 0\n",
    "usa_dis_len = 0\n",
    "usa_am_len = 0\n",
    "eu_net_len = 0\n",
    "eu_dis_len = 0\n",
    "eu_am_len = 0\n",
    "oth_net_len = 0\n",
    "oth_dis_len = 0\n",
    "oth_am_len = 0\n",
    "for index,row in data.iterrows():\n",
    "    if index in usa:\n",
    "        if index in net:\n",
    "            usa_net_sum += row['polarity']\n",
    "            usa_net_len += 1\n",
    "        if index in disney:\n",
    "            usa_dis_sum += row['polarity']\n",
    "            usa_dis_len += 1\n",
    "        if index in amazon:\n",
    "            usa_am_sum += row['polarity']\n",
    "            usa_am_len += 1\n",
    "    elif index in EU:\n",
    "        if index in net:\n",
    "            eu_net_sum += row['polarity']\n",
    "            eu_net_len +=1\n",
    "        if index in disney:\n",
    "            eu_dis_sum += row['polarity']\n",
    "            eu_dis_len += 1\n",
    "        if index in amazon:\n",
    "            eu_am_sum += row['polarity']\n",
    "            eu_am_len += 1\n",
    "    elif index in other:\n",
    "        if index in net:\n",
    "            oth_net_sum += row['polarity']\n",
    "            oth_net_len += 1\n",
    "        if index in disney:\n",
    "            oth_dis_sum += row['polarity']\n",
    "            oth_dis_len += 1\n",
    "        if index in amazon:\n",
    "            oth_am_sum += row['polarity']\n",
    "            oth_am_len += 1\n",
    "\n",
    "Loc_graph = [[usa_net_sum/usa_net_len,eu_net_sum/eu_net_len,oth_net_sum/oth_net_len],\n",
    "             [usa_dis_sum/usa_dis_len,eu_dis_sum/eu_dis_len,oth_dis_sum/oth_dis_len],\n",
    "             [usa_am_sum/usa_am_len,eu_am_sum/eu_am_len,oth_am_sum/oth_am_len]]\n",
    "\n",
    "# Plot the polarity of the tweets based on Location info\n",
    "X = np.arange(3)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylabel('Average polarity')\n",
    "ax.set_title('Average opinion for each platform in USA, EU and rest of the world')\n",
    "ax.set_xticks([0.3,1.3,2.3], ('Netflix', 'Disney Plus', 'Amazon Prime'))\n",
    "ax.bar(X + 0.00, Loc_graph[0], color = 'b', width = 0.25, label='USA')\n",
    "ax.bar(X + 0.25, Loc_graph[1], color = 'g', width = 0.25, label='EU')\n",
    "ax.bar(X + 0.50, Loc_graph[2], color = 'r', width = 0.25, label='other')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Preperation of the data in order to plot the polarity of the tweets based on time info\n",
    "time = []\n",
    "for row in data.index:\n",
    "    time.append(datetime.strptime(data['date'][row][0:19],'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "t8_net_sum = 0\n",
    "t8_dis_sum = 0\n",
    "t8_am_sum = 0\n",
    "t14_net_sum = 0\n",
    "t14_dis_sum = 0\n",
    "t14_am_sum = 0\n",
    "t24_net_sum = 0\n",
    "t24_dis_sum = 0\n",
    "t24_am_sum = 0\n",
    "t8_net_len = 0\n",
    "t8_dis_len = 0\n",
    "t8_am_len = 0\n",
    "t14_net_len = 0\n",
    "t14_dis_len = 0\n",
    "t14_am_len = 0\n",
    "t24_net_len = 0\n",
    "t24_dis_len = 0\n",
    "t24_am_len = 0\n",
    "for index,row in data.iterrows():\n",
    "    if time[index] < datetime.strptime('2022-02-25 19:00:00','%Y-%m-%d %H:%M:%S'):\n",
    "        if index in net:\n",
    "            t8_net_sum += row['polarity']\n",
    "            t8_net_len += 1\n",
    "        if index in disney:\n",
    "            t8_dis_sum += row['polarity']\n",
    "            t8_dis_len += 1\n",
    "        if index in amazon:\n",
    "            t8_am_sum += row['polarity']\n",
    "            t8_am_len += 1\n",
    "    elif time[index] < datetime.strptime('2022-02-25 20:00:00','%Y-%m-%d %H:%M:%S'):\n",
    "        if index in net:\n",
    "            t14_net_sum += row['polarity']\n",
    "            t14_net_len += 1\n",
    "        if index in disney:\n",
    "            t14_dis_sum += row['polarity']\n",
    "            t14_dis_len += 1\n",
    "        if index in amazon:\n",
    "            t14_am_sum += row['polarity']\n",
    "            t14_am_len += 1\n",
    "    else:\n",
    "        if index in net:\n",
    "            t24_net_sum += row['polarity']\n",
    "            t24_net_len += 1\n",
    "        if index in disney:\n",
    "            t24_dis_sum += row['polarity']\n",
    "            t24_dis_len += 1\n",
    "        if index in amazon:\n",
    "            t24_am_sum += row['polarity']\n",
    "            t24_am_len += 1\n",
    "\n",
    "Time_graph = [[t8_net_sum/t8_net_len,t14_net_sum/t14_net_len,t24_net_sum/t24_net_len],\n",
    "             [t8_dis_sum/t8_dis_len,t14_dis_sum/t14_dis_len,t24_dis_sum/t24_dis_len],\n",
    "             [t8_am_sum/t8_am_len,t14_am_sum/t14_am_len,t24_am_sum/t24_am_len]]\n",
    "\n",
    "# Plot the polarity of the tweets based on time info\n",
    "X = np.arange(3)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylabel('Average polarity')\n",
    "ax.set_title('Average opinion for each platform in different time periods')\n",
    "ax.set_xticks([0.3,1.3,2.3], ('before 19:00', 'before 20:00', 'after 20:00'))\n",
    "ax.bar(X + 0.00, Time_graph[0], color = 'b', width = 0.25, label='Netflix')\n",
    "ax.bar(X + 0.25, Time_graph[1], color = 'g', width = 0.25, label='Disney Plus')\n",
    "ax.bar(X + 0.50, Time_graph[2], color = 'r', width = 0.25, label='Amazon Prime')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04ba74",
   "metadata": {},
   "source": [
    "# PROXEIRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIA TUXAIA EPILOGH YPARXONTWN TWEETS\n",
    "\n",
    "searchTerm = '.....'\n",
    "noOfSearchTerms = 1000\n",
    "\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=searchTerm, lang='en').items(noOfSearchTerms)\n",
    "#each of those lists will contain the values for a column in the dataframe\n",
    "list1=[]\n",
    "list2=[]\n",
    "list3=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "list6=[]\n",
    "list7=[]\n",
    "list8=[]\n",
    "list9=[]\n",
    "\n",
    "# Pass the tweets' metadata in lists\n",
    "for tweet in tweets:\n",
    "    list1.append(tweet.text)\n",
    "    list2.append(tweet.user.screen_name)\n",
    "    list3.append(tweet.created_at)\n",
    "    list4.append(tweet.retweet_count)\n",
    "    list5.append(tweet.user.location)\n",
    "    list6.append(tweet.user.followers_count)\n",
    "    list7.append(tweet.user.statuses_count)\n",
    "    list8.append(tweet.entities['hashtags'])\n",
    "\n",
    "# Create the dataframe\n",
    "data = pd.DataFrame(list2, columns=[\"user\"])\n",
    "data.insert(loc=1, column='tweet', value=list1)\n",
    "data.insert(loc=2, column='date', value=list3)\n",
    "data.insert(loc=3, column='n_ret', value=list4)\n",
    "data.insert(loc=4, column='loc', value=list5)\n",
    "data.insert(loc=5, column='n_fol', value=list6)\n",
    "data.insert(loc=6, column='n_twe', value=list7)\n",
    "data.insert(loc=7, column='hashtags', value=list8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3179c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mporei na mpei STO ONLINE TWEETS FETCHING\n",
    "\n",
    "# Enallaktika apothikeush twn tweets kateutheian sthn database\n",
    "        try:\n",
    "            db = mysql.connector.connect(host='localhost', database='twit_sent_an', user='root', password='')\n",
    "            if db.is_connected():\n",
    "                #print('Connected to MySQL Database')\n",
    "                cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "                q = \"INSERT INTO tweets (username, n_fol, n_twe, n_ret, Tweet, T_date, Loc, Hashtags, sentiment) VALUES ('\" \\\n",
    "                + re.sub(r'[\\']', ' ', str(status.user.screen_name)) + \"', '\" + str(status.user.followers_count) + \"', '\" + str(status.user.statuses_count) + \\\n",
    "                \"', '\" + str(status.retweet_count) + \"', '\" + re.sub(r'[\\']', ' ',str(status.text)) + \"', '\" + str(status.created_at) + \\\n",
    "                \"', '\" + str(status.user.location) + \"', '\" + re.sub(r'[\\']', ' ',str(htags)) + \"', '\" + str(None) + \\\n",
    "                \"');\"\n",
    "\n",
    "                cur.execute(q)\n",
    "\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "\n",
    "        finally:\n",
    "            cur.execute(\"commit;\")\n",
    "            # cur.execute(\"SELECT * FROM tweets;\")\n",
    "            # data = cur.fetchall()\n",
    "            # df = pd.DataFrame(data, columns = columns)\n",
    "            db.close()\n",
    "            #print('Database connection closed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93430c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STREAMING IS NOT PERMITTED ON FREE VERSIONS ANY MORE !!!!\n",
    "\n",
    "# Creation of Listener class, which contains the methods we \n",
    "# want with the settings we want from the methods of \n",
    "# Streaming Client object\n",
    "\n",
    "# Na dw ti akrivws kanei h parakatw grammh\n",
    "# Dhmiourgei class me antigrafh? kanei edit mia class?\n",
    "# Epishs to exw grapsei swsta? \n",
    "class MyListener(tweepy.StreamingClient):  # Inherit class'tweepy.StreamingClient' to my new class'MyListener'\n",
    "    \n",
    "    def on_connect(self):\n",
    "        print(\"Connected!\")\n",
    "    \n",
    "    # Initialization for storing the tweets in object (list) 'data'\n",
    "    ##@@ Edw mporw na xrhsimopoihsw dataframe anti gia list???  @@@\n",
    "    \n",
    "    data = []\n",
    "    columns = ['User', 'n_fol', 'n_twe', 'n_ret', 'Tweet', 'Date', 'Loc', 'Hashtags', 'Sentiment']\n",
    "\n",
    "    def on_tweet(self, tweet):  #receives a tweet and processes it according to the conditions, if there are any, and adds the tweet to the hashmap\n",
    "        print(f\"{tweet.id} {tweet.created_at} ({tweet.author_id}): {tweet.text}\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        #enallaktika\n",
    "        if tweet.referenced_tweets == None:\n",
    "            # self.new_tweet[“tweet”] = tweet.text\n",
    "            print(tweet.text)\n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        # Perasma twn tweets se list\n",
    "        user = self.get_user(username=screen_name)\n",
    "        print(user)\n",
    "#         self.data.append([status.user.screen_name, status.user.followers_count, status.user.statuses_count,\n",
    "#                          status.retweet_count, status.text, str(status.created_at), status.user.location,\n",
    "#                          htags, None])\n",
    "    \n",
    "    def on_includes(self, includes):  #is responsible for the user details and adds the user data to the hashmap\n",
    "        self.new_tweet[\"username\"] = includes[\"users\"][0].username\n",
    "        print(self.new_tweet)\n",
    "#         # insert tweets in db\n",
    "#         cursor.execute(\n",
    "#         \"INSERT INTO tweets VALUES (?,?)\",\n",
    "#         (\n",
    "#         self.new_tweet[\"username\"],\n",
    "#         self.new_tweet[\"tweet\"],\n",
    "#         ),\n",
    "#         )\n",
    "#         conn.commit()\n",
    "#         # print(self.new_tweet)\n",
    "#         print(\"tweet added to db!\")\n",
    "#         print(\"-\" * 30)\n",
    "   \n",
    "    \n",
    "    # prints the error\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "####   APORIES\n",
    "    # Travame to created_at san string   [datetime.datetime(2022, 2, 20, 11, 5, 25, tzinfo=datetime.timezone.utc)]\n",
    "    # Travame mono to text apo ta hashtags [{'text': 'Unbelievable', 'indices': [8, 21]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e64e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONLINE TWEETS FETCHING\n",
    "new_Tweet = MyListener(bearer_token)\n",
    "# new_Tweet = tweepy.StreamingClient(bearer_token)\n",
    "\n",
    "new_Tweet.sample()\n",
    "\n",
    "# add new rules    \n",
    "# rule = tweepy.StreamRule(value=\"Python\")\n",
    "# new_Tweet.add_rules(rule)\n",
    " \n",
    "# new_Tweet.filter()\n",
    "# new_Tweet.filter(expansions=\"author_id\",tweet_fields=\"created_at\")\n",
    "\n",
    "\n",
    "# add the keywords for the streamer. It returns JSON formatted dictionary object\n",
    "# new_Tweet.filter(languages=['en', 'gr'], track=['fires', 'Greece'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
