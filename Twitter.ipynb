{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1387be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR ONE TIME RUN\n",
    "\n",
    "# for tweepy we need to add 'conda-forge' channel in the anaconda environment and update (channels). Then, we can find tweepy in the anaconda list of packages\n",
    "# for mysql.connector I add 'mysql-connector-python'\n",
    "# for matplotlib I can use the alternative way of intalling: I write 'pip install matplotlib' in conda prompt\n",
    "# for textblob I write in prompt 'pip install textblob'\n",
    "# for pandas I write in prompt 'pip install pandas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c965d3c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Stream' from 'tweepy' (C:\\Users\\Kosand\\anaconda3\\envs\\Case_studies\\lib\\site-packages\\tweepy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtweepy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OAuthHandler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtweepy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m API\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtweepy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Stream\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmysql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmysql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Error   \u001b[38;5;66;03m#Gia na fortwsw ta eidika errors gia sql connection\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Stream' from 'tweepy' (C:\\Users\\Kosand\\anaconda3\\envs\\Case_studies\\lib\\site-packages\\tweepy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys  # ??\n",
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Stream\n",
    "import mysql.connector\n",
    "from mysql.connector import Error   #Gia na fortwsw ta eidika errors gia sql connection\n",
    "import configparser  # ??\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import string  # ??\n",
    "import nltk  # ??\n",
    "from nltk.stem import WordNetLemmatizer  # ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af4b59",
   "metadata": {},
   "source": [
    "# Preparation of Twitter Listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9229b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configuration of the twitter account\n",
    "api_key = 'SbPzmphYe7DHRMB0QPnYCuj1D'\n",
    "api_key_secret = 'Wod9VZS49DHxKhXKc5TL6yO2yrzZpeakvUUH1bufrspbBt8ygh'\n",
    "access_token = '1485552804655685639-5J3YnASSM9BwL50v8ZpNsjVURHSmV6'\n",
    "access_token_secret = 'p6xUzi3moQ5GliaHm2MTQphzPGiz955SirqYzAdF6qnPd'\n",
    "\n",
    "# Authentication of Twitter API\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93430c74",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tweepy' has no attribute 'Stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m configparser\u001b[38;5;241m.\u001b[39mConfigParser()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(api_key)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyListener\u001b[39;00m(\u001b[43mtweepy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStream\u001b[49m):\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Initialization for storing the tweets in object (list) 'data'\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m##@@ Edw mporw na xrhsimopoihsw dataframe anti gia list???  @@@\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_fol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_twe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_ret\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHashtags\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tweepy' has no attribute 'Stream'"
     ]
    }
   ],
   "source": [
    "# ONLINE TWEETS FETCHING\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "# print(api_key)\n",
    "\n",
    "class MyListener(tweepy.Stream):\n",
    "\n",
    "    # Initialization for storing the tweets in object (list) 'data'\n",
    "    ##@@ Edw mporw na xrhsimopoihsw dataframe anti gia list???  @@@\n",
    "    \n",
    "    data = []\n",
    "    columns = ['User', 'n_fol', 'n_twe', 'n_ret', 'Tweet', 'Date', 'Loc', 'Hashtags', 'Sentiment']\n",
    "\n",
    "    def on_status(self, status):\n",
    "        htags = status.entities.get('hashtags')\n",
    "        if len(htags) != 0:\n",
    "            htags = htags[0]\n",
    "            htags = htags['text']\n",
    "        else:\n",
    "            htags = ''\n",
    "        # print(status.created_at)\n",
    "        # self.data.append(str(status.created_at))\n",
    "\n",
    "        # Perasma twn tweets se list\n",
    "        self.data.append([status.user.screen_name, status.user.followers_count, status.user.statuses_count,\n",
    "                         status.retweet_count, status.text, str(status.created_at), status.user.location,\n",
    "                         htags, None])\n",
    "\n",
    "    # prints the error\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "####   APORIES\n",
    "    # Travame to created_at san string   [datetime.datetime(2022, 2, 20, 11, 5, 25, tzinfo=datetime.timezone.utc)]\n",
    "    # Travame mono to text apo ta hashtags [{'text': 'Unbelievable', 'indices': [8, 21]}]\n",
    "\n",
    "\n",
    "new_Tweet = MyListener(api_key, api_key_secret, access_token, access_token_secret)\n",
    "\n",
    "# add the keywords for the streamer. It returns JSON formatted dictionary object\n",
    "new_Tweet.filter(languages=['en', 'gr'], track=['fires', 'Greece'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5a273",
   "metadata": {},
   "source": [
    "# CONNECT TO DATABASE TO STORE RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c292b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'case_st_twit'\n",
    "\n",
    "# CONNECT TO DATABASE AND CREATE THE TABLE THAT WE WILL STORE THE TWEETS\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database = db_name , user='root', password='')\n",
    "    if db.is_connected():\n",
    "        print('Connected to MySQL Database')\n",
    "        cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "        # Creating the table of database\n",
    "        q = \"CREATE table r_tweets( \\\n",
    "                username VARCHAR(30) PRIMARY KEY, \\\n",
    "                tweet VARCHAR(280), \\\n",
    "                t_date VARCHAR(30), \\\n",
    "                n_ret INT(3), \\\n",
    "                loc VARCHAR(20), \\\n",
    "                n_fol INT(9), \\\n",
    "                n_twe INT(3), \\\n",
    "                hashtags VARCHAR(50), \\\n",
    "                commit;\"\n",
    "        cur.execute(q)\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n",
    "    print('Database connection closed!')\n",
    "\n",
    "# CONNECT TO DATABASE TO STORE THE FETCHED TWEETS AS RAW DATA\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database= db_name, user='root', password='')\n",
    "    if db.is_connected():\n",
    "        cur = db.cursor()\n",
    "\n",
    "        for row in range(len(list1)):\n",
    "            # Before running the sql commands, we clean the text data from apostrophes <'>, in order to avoid losing data due to faulty comments in sql commands\n",
    "            q = \"INSERT INTO tweets (username, tweet, t_date, n_ret, loc, n_fol, n_twe, hashtags) VALUES ('\" \\\n",
    "            + re.sub(r'[\\']', ' ',str(list2[row])) + \"', '\" + re.sub(r'[\\']', ' ',str(list1[row])) + \"', '\" + str(list3[row]) + \\\n",
    "            \"', '\" + str(list4[row]) + \"', '\" + re.sub(r'[\\']', ' ',str(list5[row])) + \"', '\" + str(list6[row]) + \\\n",
    "            \"', '\" + str(list7[row]) + \"', '\"  + re.sub(r'[\\']', ' ',str(list8[row])) + \\\n",
    "            \"');\"\n",
    "\n",
    "            try:\n",
    "                print(q)\n",
    "                cur.execute(q)\n",
    "            except Error as e:\n",
    "                print(e)\n",
    "\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831fb35b",
   "metadata": {},
   "source": [
    "# CLEANING OF THE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33933c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data from database\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database = db_name, user='root', password='')\n",
    "\n",
    "    # Initialization for storing the tweets in pandas object 'data'\n",
    "    data = []\n",
    "    columns = ['user', 'tweet', 'date', 'n_ret', 'loc', 'n_fol', 'n_twe', 'hashtags']\n",
    "    if db.is_connected():\n",
    "        cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "        q = \"SELECT * FROM tweets;\"\n",
    "        cur.execute(q)\n",
    "        #Create the dataframe with all the data from database\n",
    "        data = pd.DataFrame(cur.fetchall(), columns=columns)\n",
    "\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n",
    "\n",
    "\n",
    "# Remove duplicate users. We want one tweet from every user\n",
    "data.drop_duplicates(subset =\"user\", keep = False, inplace = True)\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "list9=[]\n",
    "for row in data.index:\n",
    "    # Remove punctuations\n",
    "    cleaned_tweet = ''.join(re.sub (\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])| (\\w+:\\ / \\ / \\S+)\", \" \", data['tweet'][row]))\n",
    "    # Remove the words with the @ symbol\n",
    "    cleaned_tweet = re.sub ('@\\S+', '', cleaned_tweet)\n",
    "    # Remove the Url's\n",
    "    cleaned_tweet = re.sub('((www.[^s]+)|(https?://[^s]+))', ' ', cleaned_tweet)\n",
    "    # Remove the hashtags\n",
    "    cleaned_tweet = re.sub ('#\\S+', '', cleaned_tweet)\n",
    "    # Remove the change row\n",
    "    cleaned_tweet = re.sub ('\\\\n', '', cleaned_tweet)\n",
    "    # Remove Stopwords\n",
    "    stopwords = set([\"\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"A\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"])\n",
    "    cleaned_tweet = ' '.join([word for word in str(cleaned_tweet).split() if word not in stopwords])\n",
    "\n",
    "    # Calculate the polarity of the sentiment analysis using textblob and store the results in the sentiment column of the dataframe\n",
    "    analysis = TextBlob(cleaned_tweet)\n",
    "    list9.append(analysis.sentiment.polarity)\n",
    "\n",
    "    if (analysis.sentiment.polarity == 0):\n",
    "        neutral += 1\n",
    "    elif (analysis.sentiment.polarity < 0):\n",
    "        negative += 1\n",
    "    elif (analysis.sentiment.polarity > 0):\n",
    "        positive += 1\n",
    "\n",
    "data.insert(loc=8, column='polarity', value = list9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b224cbfe",
   "metadata": {},
   "source": [
    "# STORE THE CLEANED TWEETS IN DATABASE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a856888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A NEW TABLE IN DATABASE cl_tweets\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database='twit_sent_an', user='root', password='')\n",
    "    if db.is_connected():\n",
    "        cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "        q = \"CREATE table cl_tweets( \\\n",
    "                username VARCHAR(30) PRIMARY KEY, \\\n",
    "                tweet VARCHAR(280), \\\n",
    "                t_date VARCHAR(30), \\\n",
    "                n_ret INT(3), \\\n",
    "                loc VARCHAR(20), \\\n",
    "                n_fol INT(9), \\\n",
    "                n_twe INT(3), \\\n",
    "                hashtags VARCHAR(50), \\\n",
    "                sentiment FLOAT(4,3)); \\\n",
    "                commit;\"\n",
    "        cur.execute(q)\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n",
    "\n",
    "\n",
    "# STORE THE CLEANED TWEETS WITH THE SENTIMENT ANALYSIS RESULTS IN A NEW TABLE IN DATABASE cl_tweets\n",
    "try:\n",
    "    db = mysql.connector.connect(host='localhost', database='twit_sent_an', user='root', password='')\n",
    "    if db.is_connected():\n",
    "        print('Connected to MySQL Database')\n",
    "        cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "        for row in data.index:\n",
    "            q = \"INSERT INTO cl_tweets (username, tweet, t_date, n_ret, loc, n_fol, n_twe, hashtags, sentiment) VALUES ('\" \\\n",
    "            + str(data['user'][row]) + \"', '\" + str(data['tweet'][row]) + \"', '\" + str(data['date'][row]) + \\\n",
    "            \"', '\" + str(data['n_ret'][row]) + \"', '\" + str(data['loc'][row]) + \"', '\" + str(data['n_fol'][row]) + \\\n",
    "            \"', '\" + str(data['n_twe'][row]) + \"', '\"  + str(data['hashtags'][row]) + \\\n",
    "            \"', '\" + str(data['polarity'][row]) + \"');\"\n",
    "\n",
    "            cur.execute(q)\n",
    "\n",
    "except Error as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    cur.execute(\"commit;\")\n",
    "    db.close()\n",
    "    print('Database connection closed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mega_positive=0\n",
    "mega_negative=0\n",
    "mega_neutral=0\n",
    "mini_positive=0\n",
    "mini_negative=0\n",
    "mini_neutral=0\n",
    "\n",
    "list10=[]\n",
    "#list 10 is going to accept 2 values (1 for the people that have more followers than average and 0\n",
    "#for those who have followers less than average)\n",
    "mean_followers = sum(data['n_fol'])/ len(data['n_fol'])\n",
    "for element in data['n_fol']:\n",
    "    if element > mean_followers:\n",
    "        list10.append(1)\n",
    "    else:\n",
    "        list10.append(0)\n",
    "\n",
    "data.insert(loc=9, column='weight', value=list10)\n",
    "\n",
    "# 9th and 10th column in the dataframe contain the polarity value and the corresponding weight whether\n",
    "#a user has followers above average or not\n",
    "#So mega_positive will get +1 when we meet a user that has followers above average and polarity > 0\n",
    "#Thus, mini positive will get +1 when we meet a user that has followers below average and polarity > 0 etc.\n",
    "for index,row in data.iterrows():\n",
    "    if row['polarity'] > 0 and row['weight'] == 1:\n",
    "        mega_positive += 1\n",
    "    elif row['polarity'] > 0 and row['weight'] == 0:\n",
    "        mini_positive += 1\n",
    "    elif row['polarity'] < 0 and row['weight'] == 1:\n",
    "        mega_negative += 1\n",
    "    elif row['polarity'] < 0 and row['weight'] == 0:\n",
    "        mini_negative += 1\n",
    "    elif row['polarity'] == 0 and row['weight'] == 1:\n",
    "        mega_neutral += 1\n",
    "    else:\n",
    "        mini_neutral += 1\n",
    "\n",
    "\n",
    "#we want to have the percentage each opinion covers in the pie we are going to create\n",
    "positive =(positive/ noOfSearchTerms)*100\n",
    "negative =(negative/ noOfSearchTerms)*100\n",
    "neutral =(neutral/ noOfSearchTerms)*100\n",
    "\n",
    "final_positive = mega_positive*0.7 + mini_positive*0.3\n",
    "final_negative = mega_negative*0.7 + mini_negative*0.3\n",
    "final_neutral = mega_neutral*0.7 + mini_neutral*0.3\n",
    "\n",
    "\n",
    "print(\"how people are reacting on \" + searchTerm + \"by analyzing\" + str(noOfSearchTerms) + \"Tweets.\")\n",
    "\n",
    "if (polarity == 0):\n",
    "    print(\"Neutral\")\n",
    "elif (polarity < 0):\n",
    "    print(\"Negative\")\n",
    "else:\n",
    "    print(\"Positive\")\n",
    "\n",
    "#plotting the polarity of objects\n",
    "labels = ['Positive [' +str(positive)+'%]', 'Neutral [' +str(neutral)+ '%]', 'Negative [' +str(negative) + '%]']\n",
    "\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['green', 'yellow', 'red']\n",
    "patches, text = plt.pie(sizes, colors=colors)\n",
    "\n",
    "plt.legend(patches, labels, loc='best')\n",
    "plt.title('what people think about ' +searchTerm+'by analyzing '+str(noOfSearchTerms)+'Tweets.')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#plotting the impact that people with more followers have on the objects\n",
    "labels1 = ['Positive ' +str(final_positive)+' impact', 'Neutral ' +str(final_neutral)+ ' impact', 'Negative ' +str(final_negative) + ' impact']\n",
    "\n",
    "sizes1 = [final_positive, final_neutral, final_negative]\n",
    "colors1= ['green', 'yellow', 'red']\n",
    "patches1, text1 = plt.pie(sizes, colors=colors)\n",
    "\n",
    "plt.legend(patches1, labels1, loc='best')\n",
    "plt.title('what people think about ' +searchTerm+'by analyzing '+str(noOfSearchTerms)+'Tweets.')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Dataset separation based on the location. Here we use two csv's tha contain some cities in USA and Europe\n",
    "abbUS = pd.read_csv('AbbrevUSA.csv')\n",
    "abbEU = pd.read_csv('AbbrevEU.txt')\n",
    "usa = []\n",
    "EU = []\n",
    "other = []\n",
    "for index,row in data.iterrows():\n",
    "    if row['loc'] in abbUS.values:\n",
    "        usa.append(index)\n",
    "    elif row['loc'] in abbEU.values:\n",
    "        EU.append(index)\n",
    "    elif row['loc']!= '':\n",
    "        other.append(index)\n",
    "\n",
    "# Dataset separation based on the platform\n",
    "net = []\n",
    "disney = []\n",
    "amazon = []\n",
    "net_sum = 0\n",
    "dis_sum = 0\n",
    "am_sum = 0\n",
    "for index,row in data.iterrows():\n",
    "    if 'netflix' in row['tweet'].lower():\n",
    "        net.append(index)\n",
    "        net_sum += row['polarity']\n",
    "    elif 'disney' in row['tweet'].lower():\n",
    "        disney.append(index)\n",
    "        dis_sum += row['polarity']\n",
    "    elif 'amazon' in row['tweet'].lower():\n",
    "        amazon.append(index)\n",
    "        am_sum += row['polarity']\n",
    "\n",
    "# Preperation of the data in order to plot the polarity of the tweets based on Location info\n",
    "usa_net_sum = 0\n",
    "usa_dis_sum = 0\n",
    "usa_am_sum = 0\n",
    "eu_net_sum = 0\n",
    "eu_dis_sum = 0\n",
    "eu_am_sum = 0\n",
    "oth_net_sum = 0\n",
    "oth_dis_sum = 0\n",
    "oth_am_sum = 0\n",
    "usa_net_len = 0\n",
    "usa_dis_len = 0\n",
    "usa_am_len = 0\n",
    "eu_net_len = 0\n",
    "eu_dis_len = 0\n",
    "eu_am_len = 0\n",
    "oth_net_len = 0\n",
    "oth_dis_len = 0\n",
    "oth_am_len = 0\n",
    "for index,row in data.iterrows():\n",
    "    if index in usa:\n",
    "        if index in net:\n",
    "            usa_net_sum += row['polarity']\n",
    "            usa_net_len += 1\n",
    "        if index in disney:\n",
    "            usa_dis_sum += row['polarity']\n",
    "            usa_dis_len += 1\n",
    "        if index in amazon:\n",
    "            usa_am_sum += row['polarity']\n",
    "            usa_am_len += 1\n",
    "    elif index in EU:\n",
    "        if index in net:\n",
    "            eu_net_sum += row['polarity']\n",
    "            eu_net_len +=1\n",
    "        if index in disney:\n",
    "            eu_dis_sum += row['polarity']\n",
    "            eu_dis_len += 1\n",
    "        if index in amazon:\n",
    "            eu_am_sum += row['polarity']\n",
    "            eu_am_len += 1\n",
    "    elif index in other:\n",
    "        if index in net:\n",
    "            oth_net_sum += row['polarity']\n",
    "            oth_net_len += 1\n",
    "        if index in disney:\n",
    "            oth_dis_sum += row['polarity']\n",
    "            oth_dis_len += 1\n",
    "        if index in amazon:\n",
    "            oth_am_sum += row['polarity']\n",
    "            oth_am_len += 1\n",
    "\n",
    "Loc_graph = [[usa_net_sum/usa_net_len,eu_net_sum/eu_net_len,oth_net_sum/oth_net_len],\n",
    "             [usa_dis_sum/usa_dis_len,eu_dis_sum/eu_dis_len,oth_dis_sum/oth_dis_len],\n",
    "             [usa_am_sum/usa_am_len,eu_am_sum/eu_am_len,oth_am_sum/oth_am_len]]\n",
    "\n",
    "# Plot the polarity of the tweets based on Location info\n",
    "X = np.arange(3)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylabel('Average polarity')\n",
    "ax.set_title('Average opinion for each platform in USA, EU and rest of the world')\n",
    "ax.set_xticks([0.3,1.3,2.3], ('Netflix', 'Disney Plus', 'Amazon Prime'))\n",
    "ax.bar(X + 0.00, Loc_graph[0], color = 'b', width = 0.25, label='USA')\n",
    "ax.bar(X + 0.25, Loc_graph[1], color = 'g', width = 0.25, label='EU')\n",
    "ax.bar(X + 0.50, Loc_graph[2], color = 'r', width = 0.25, label='other')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Preperation of the data in order to plot the polarity of the tweets based on time info\n",
    "time = []\n",
    "for row in data.index:\n",
    "    time.append(datetime.strptime(data['date'][row][0:19],'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "t8_net_sum = 0\n",
    "t8_dis_sum = 0\n",
    "t8_am_sum = 0\n",
    "t14_net_sum = 0\n",
    "t14_dis_sum = 0\n",
    "t14_am_sum = 0\n",
    "t24_net_sum = 0\n",
    "t24_dis_sum = 0\n",
    "t24_am_sum = 0\n",
    "t8_net_len = 0\n",
    "t8_dis_len = 0\n",
    "t8_am_len = 0\n",
    "t14_net_len = 0\n",
    "t14_dis_len = 0\n",
    "t14_am_len = 0\n",
    "t24_net_len = 0\n",
    "t24_dis_len = 0\n",
    "t24_am_len = 0\n",
    "for index,row in data.iterrows():\n",
    "    if time[index] < datetime.strptime('2022-02-25 19:00:00','%Y-%m-%d %H:%M:%S'):\n",
    "        if index in net:\n",
    "            t8_net_sum += row['polarity']\n",
    "            t8_net_len += 1\n",
    "        if index in disney:\n",
    "            t8_dis_sum += row['polarity']\n",
    "            t8_dis_len += 1\n",
    "        if index in amazon:\n",
    "            t8_am_sum += row['polarity']\n",
    "            t8_am_len += 1\n",
    "    elif time[index] < datetime.strptime('2022-02-25 20:00:00','%Y-%m-%d %H:%M:%S'):\n",
    "        if index in net:\n",
    "            t14_net_sum += row['polarity']\n",
    "            t14_net_len += 1\n",
    "        if index in disney:\n",
    "            t14_dis_sum += row['polarity']\n",
    "            t14_dis_len += 1\n",
    "        if index in amazon:\n",
    "            t14_am_sum += row['polarity']\n",
    "            t14_am_len += 1\n",
    "    else:\n",
    "        if index in net:\n",
    "            t24_net_sum += row['polarity']\n",
    "            t24_net_len += 1\n",
    "        if index in disney:\n",
    "            t24_dis_sum += row['polarity']\n",
    "            t24_dis_len += 1\n",
    "        if index in amazon:\n",
    "            t24_am_sum += row['polarity']\n",
    "            t24_am_len += 1\n",
    "\n",
    "Time_graph = [[t8_net_sum/t8_net_len,t14_net_sum/t14_net_len,t24_net_sum/t24_net_len],\n",
    "             [t8_dis_sum/t8_dis_len,t14_dis_sum/t14_dis_len,t24_dis_sum/t24_dis_len],\n",
    "             [t8_am_sum/t8_am_len,t14_am_sum/t14_am_len,t24_am_sum/t24_am_len]]\n",
    "\n",
    "# Plot the polarity of the tweets based on time info\n",
    "X = np.arange(3)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylabel('Average polarity')\n",
    "ax.set_title('Average opinion for each platform in different time periods')\n",
    "ax.set_xticks([0.3,1.3,2.3], ('before 19:00', 'before 20:00', 'after 20:00'))\n",
    "ax.bar(X + 0.00, Time_graph[0], color = 'b', width = 0.25, label='Netflix')\n",
    "ax.bar(X + 0.25, Time_graph[1], color = 'g', width = 0.25, label='Disney Plus')\n",
    "ax.bar(X + 0.50, Time_graph[2], color = 'r', width = 0.25, label='Amazon Prime')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04ba74",
   "metadata": {},
   "source": [
    "# PROXEIRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIA TUXAIA EPILOGH YPARXONTWN TWEETS\n",
    "\n",
    "searchTerm = '.....'\n",
    "noOfSearchTerms = 1000\n",
    "\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=searchTerm, lang='en').items(noOfSearchTerms)\n",
    "#each of those lists will contain the values for a column in the dataframe\n",
    "list1=[]\n",
    "list2=[]\n",
    "list3=[]\n",
    "list4=[]\n",
    "list5=[]\n",
    "list6=[]\n",
    "list7=[]\n",
    "list8=[]\n",
    "list9=[]\n",
    "\n",
    "# Pass the tweets' metadata in lists\n",
    "for tweet in tweets:\n",
    "    list1.append(tweet.text)\n",
    "    list2.append(tweet.user.screen_name)\n",
    "    list3.append(tweet.created_at)\n",
    "    list4.append(tweet.retweet_count)\n",
    "    list5.append(tweet.user.location)\n",
    "    list6.append(tweet.user.followers_count)\n",
    "    list7.append(tweet.user.statuses_count)\n",
    "    list8.append(tweet.entities['hashtags'])\n",
    "\n",
    "# Create the dataframe\n",
    "data = pd.DataFrame(list2, columns=[\"user\"])\n",
    "data.insert(loc=1, column='tweet', value=list1)\n",
    "data.insert(loc=2, column='date', value=list3)\n",
    "data.insert(loc=3, column='n_ret', value=list4)\n",
    "data.insert(loc=4, column='loc', value=list5)\n",
    "data.insert(loc=5, column='n_fol', value=list6)\n",
    "data.insert(loc=6, column='n_twe', value=list7)\n",
    "data.insert(loc=7, column='hashtags', value=list8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3179c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mporei na mpei STO ONLINE TWEETS FETCHING\n",
    "\n",
    "# Enallaktika apothikeush twn tweets kateutheian sthn database\n",
    "        try:\n",
    "            db = mysql.connector.connect(host='localhost', database='twit_sent_an', user='root', password='')\n",
    "            if db.is_connected():\n",
    "                #print('Connected to MySQL Database')\n",
    "                cur = db.cursor()  # create object cursor from cursor method\n",
    "\n",
    "                q = \"INSERT INTO tweets (username, n_fol, n_twe, n_ret, Tweet, T_date, Loc, Hashtags, sentiment) VALUES ('\" \\\n",
    "                + re.sub(r'[\\']', ' ', str(status.user.screen_name)) + \"', '\" + str(status.user.followers_count) + \"', '\" + str(status.user.statuses_count) + \\\n",
    "                \"', '\" + str(status.retweet_count) + \"', '\" + re.sub(r'[\\']', ' ',str(status.text)) + \"', '\" + str(status.created_at) + \\\n",
    "                \"', '\" + str(status.user.location) + \"', '\" + re.sub(r'[\\']', ' ',str(htags)) + \"', '\" + str(None) + \\\n",
    "                \"');\"\n",
    "\n",
    "                cur.execute(q)\n",
    "\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "\n",
    "        finally:\n",
    "            cur.execute(\"commit;\")\n",
    "            # cur.execute(\"SELECT * FROM tweets;\")\n",
    "            # data = cur.fetchall()\n",
    "            # df = pd.DataFrame(data, columns = columns)\n",
    "            db.close()\n",
    "            #print('Database connection closed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
